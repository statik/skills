name: Validate Skills

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

permissions:
  contents: read
  pull-requests: write

env:
  PYTHONUNBUFFERED: "1"

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install skills-ref
        run: |
          pip install skills-ref
          echo "Installed to: $(pip show skills-ref | grep Location)"

      - name: Find and validate skills
        run: |
          exit_code=0
          for skill_dir in $(find . -name "SKILL.md" -exec dirname {} \; | sort -u); do
            echo "Validating: $skill_dir"
            if ! python -c "import sys; sys.argv = ['skills-ref', 'validate', '$skill_dir']; from skills_ref.cli import main; main()"; then
              exit_code=1
            fi
          done
          exit $exit_code

  run-evals:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        working-directory: evals
        run: uv sync

      - name: Check API key
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          if [ -z "$ANTHROPIC_API_KEY" ]; then
            echo "::warning::ANTHROPIC_API_KEY secret is not set. Evals will be skipped."
            echo "SKIP_EVALS=true" >> $GITHUB_ENV
          else
            echo "ANTHROPIC_API_KEY is configured"
            echo "SKIP_EVALS=false" >> $GITHUB_ENV
          fi

      - name: Run InspectAI evals
        if: env.SKIP_EVALS != 'true'
        working-directory: evals
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Create log directory
          mkdir -p logs

          # Start DNS server in background and run evals
          uv run python << 'PYTHON_SCRIPT'
          import subprocess
          import time
          import os
          import sys

          from dns_server import TestDNSServer
          from test_zones import get_all_zones

          # Start DNS server
          zones = get_all_zones()
          server = TestDNSServer(zones, port=5053)
          server.start()
          time.sleep(0.5)
          print("DNS server started on port 5053", flush=True)

          try:
              # Run the InspectAI eval with explicit log directory
              cmd = [
                  "uv", "run", "inspect", "eval", "dns_skill_eval.py",
                  "--model", "anthropic/claude-sonnet-4-20250514",
                  "--log-dir", "./logs",
                  "--display", "none"
              ]
              print(f"Running: {' '.join(cmd)}", flush=True)

              result = subprocess.run(cmd, timeout=600)
              eval_exit_code = result.returncode
          except subprocess.TimeoutExpired:
              print("Eval timed out after 600 seconds", flush=True)
              eval_exit_code = 1
          except Exception as e:
              print(f"Error running eval: {e}", flush=True)
              eval_exit_code = 1
          finally:
              server.stop()
              print("DNS server stopped", flush=True)

          # List log files for debugging
          import pathlib
          log_dir = pathlib.Path("./logs")
          if log_dir.exists():
              print(f"Log files in {log_dir}:", flush=True)
              for f in log_dir.rglob("*"):
                  print(f"  {f}", flush=True)
          else:
              print("Log directory does not exist", flush=True)

          sys.exit(eval_exit_code)
          PYTHON_SCRIPT

      - name: Parse and summarize eval results
        if: always()
        working-directory: evals
        run: |
          uv run python << 'PYTHON_SCRIPT'
          import json
          import os
          from pathlib import Path
          from datetime import datetime

          # Check if evals were skipped
          if os.environ.get("SKIP_EVALS") == "true":
              summary = "## DNS Troubleshooter Eval Results\n\n"
              summary += "**Status:** Skipped (ANTHROPIC_API_KEY not configured)\n\n"
              summary += "To run evals, add the `ANTHROPIC_API_KEY` secret to this repository.\n"
              with open(os.environ.get("GITHUB_STEP_SUMMARY", "summary.md"), "a") as f:
                  f.write(summary)
              with open("summary.md", "w") as f:
                  f.write(summary)
              exit(0)

          # Find log files in the explicit log directory
          log_dir = Path("./logs")
          print(f"Looking for logs in: {log_dir.absolute()}", flush=True)

          if log_dir.exists():
              print(f"Contents of {log_dir}:", flush=True)
              for item in log_dir.rglob("*"):
                  print(f"  {item}", flush=True)

          # Look for .eval files (InspectAI log format)
          log_files = list(log_dir.rglob("*.eval")) if log_dir.exists() else []

          # Also check for .json files as fallback
          if not log_files:
              log_files = list(log_dir.rglob("*.json")) if log_dir.exists() else []

          if not log_files:
              summary = "## DNS Troubleshooter Eval Results\n\n"
              summary += "**Status:** No eval logs found\n\n"
              summary += "Check the workflow logs for errors during eval execution.\n"
              with open(os.environ.get("GITHUB_STEP_SUMMARY", "summary.md"), "a") as f:
                  f.write(summary)
              with open("summary.md", "w") as f:
                  f.write(summary)
              exit(0)

          # Get the most recent log
          log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
          log_file = log_files[0]

          print(f"Reading log file: {log_file}", flush=True)

          with open(log_file) as f:
              log_data = json.load(f)

          # Extract results
          results = log_data.get("results", {})
          samples = log_data.get("samples", [])
          status = log_data.get("status", "unknown")
          error = log_data.get("error", None)

          total = len(samples)
          correct = sum(1 for s in samples if s.get("scores", {}).get("model_graded_fact", {}).get("value") == "C")
          incorrect = sum(1 for s in samples if s.get("scores", {}).get("model_graded_fact", {}).get("value") == "I")

          # Build summary
          summary = "## DNS Troubleshooter Eval Results\n\n"

          if error:
              summary += f"**Status:** Error - {error.get('message', 'Unknown error')}\n\n"
          elif total > 0:
              pct = (correct / total) * 100
              summary += f"**Score:** {correct}/{total} ({pct:.1f}%)\n\n"
          else:
              summary += f"**Status:** {status} - No samples evaluated\n\n"

          if samples:
              summary += "| Sample | Category | Expected | Result |\n"
              summary += "|--------|----------|----------|--------|\n"

              for sample in samples:
                  sample_id = sample.get("id", "unknown")
                  metadata = sample.get("metadata", {})
                  category = metadata.get("category", "unknown")
                  expected = metadata.get("expected_diagnosis", "unknown")

                  score = sample.get("scores", {}).get("model_graded_fact", {})
                  value = score.get("value", "?")

                  if value == "C":
                      result_icon = "✅"
                  elif value == "I":
                      result_icon = "❌"
                  else:
                      result_icon = "⚠️"

                  summary += f"| {sample_id} | {category} | {expected} | {result_icon} |\n"

          summary += f"\n*Eval run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

          # Write to GitHub Actions summary
          with open(os.environ.get("GITHUB_STEP_SUMMARY", "/dev/null"), "a") as f:
              f.write(summary)

          # Write to file for PR comment
          with open("summary.md", "w") as f:
              f.write(summary)

          print(summary, flush=True)
          PYTHON_SCRIPT

      - name: Upload eval logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: inspect-eval-logs
          path: |
            evals/logs/
            evals/summary.md
          if-no-files-found: ignore

      - name: Find existing comment
        if: github.event_name == 'pull_request' && always()
        uses: peter-evans/find-comment@v3
        id: find-comment
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-includes: "## DNS Troubleshooter Eval Results"

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-path: evals/summary.md
          comment-id: ${{ steps.find-comment.outputs.comment-id }}
          edit-mode: replace
