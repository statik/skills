name: Validate Skills

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

permissions:
  contents: read
  pull-requests: write

env:
  PYTHONUNBUFFERED: "1"

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install just
        uses: extractions/setup-just@v2

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Validate skills
        run: just validate

  run-evals:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install just
        uses: extractions/setup-just@v2

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        run: just setup

      - name: Check API key
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          if [ -z "$ANTHROPIC_API_KEY" ]; then
            echo "::warning::ANTHROPIC_API_KEY secret is not set. Evals will be skipped."
            echo "SKIP_EVALS=true" >> $GITHUB_ENV
          else
            echo "ANTHROPIC_API_KEY is configured"
            echo "SKIP_EVALS=false" >> $GITHUB_ENV
          fi

      - name: Run InspectAI evals
        if: env.SKIP_EVALS != 'true'
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          mkdir -p evals/logs
          just test anthropic/claude-sonnet-4-5-20250929 ./logs none

      - name: Parse and summarize eval results
        if: always()
        working-directory: evals
        run: |
          uv run python << 'PYTHON_SCRIPT'
          import json
          import zipfile
          import os
          from pathlib import Path
          from datetime import datetime

          # Check if evals were skipped
          if os.environ.get("SKIP_EVALS") == "true":
              summary = "## DNS Troubleshooter Eval Results\n\n"
              summary += "**Status:** Skipped (ANTHROPIC_API_KEY not configured)\n\n"
              summary += "To run evals, add the `ANTHROPIC_API_KEY` secret to this repository.\n"
              with open(os.environ.get("GITHUB_STEP_SUMMARY", "summary.md"), "a") as f:
                  f.write(summary)
              with open("summary.md", "w") as f:
                  f.write(summary)
              exit(0)

          # Find log files in the explicit log directory
          log_dir = Path("./logs")
          print(f"Looking for logs in: {log_dir.absolute()}", flush=True)

          if log_dir.exists():
              print(f"Contents of {log_dir}:", flush=True)
              for item in log_dir.rglob("*"):
                  print(f"  {item}", flush=True)

          # Look for .eval files (InspectAI log format - ZIP archives)
          log_files = list(log_dir.rglob("*.eval")) if log_dir.exists() else []

          # Also check for .json files as fallback
          if not log_files:
              log_files = list(log_dir.rglob("*.json")) if log_dir.exists() else []

          if not log_files:
              summary = "## DNS Troubleshooter Eval Results\n\n"
              summary += "**Status:** âŒ No eval logs found\n\n"
              summary += "Check the workflow logs for errors during eval execution.\n"
              with open(os.environ.get("GITHUB_STEP_SUMMARY", "summary.md"), "a") as f:
                  f.write(summary)
              with open("summary.md", "w") as f:
                  f.write(summary)
              exit(1)  # Fail if no logs found when evals should have run

          # Get the most recent log
          log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
          log_file = log_files[0]

          print(f"Reading log file: {log_file}", flush=True)

          # .eval files are ZIP archives with header.json and summaries.json
          if log_file.suffix == ".eval":
              with zipfile.ZipFile(log_file, "r") as zf:
                  # Read header for status
                  with zf.open("header.json") as f:
                      header = json.load(f)
                  # Read summaries for sample results
                  with zf.open("summaries.json") as f:
                      samples = json.load(f)
                  status = header.get("status", "unknown")
                  error = header.get("error", None)
          else:
              with open(log_file) as f:
                  log_data = json.load(f)
              samples = log_data.get("samples", [])
              status = log_data.get("status", "unknown")
              error = log_data.get("error", None)

          total = len(samples)
          correct = sum(1 for s in samples if s.get("scores", {}).get("model_graded_fact", {}).get("value") == "C")
          incorrect = sum(1 for s in samples if s.get("scores", {}).get("model_graded_fact", {}).get("value") == "I")
          errored = sum(1 for s in samples if s.get("error"))

          # Build summary
          summary = "## DNS Troubleshooter Eval Results\n\n"

          # Check for sample-level errors (e.g., API auth failures)
          sample_errors = [s.get("error") for s in samples if s.get("error")]
          if sample_errors:
              first_error = sample_errors[0]
              if "401" in str(first_error) or "authentication" in str(first_error).lower():
                  summary += "**Status:** Authentication Error\n\n"
                  summary += "The `ANTHROPIC_API_KEY` secret appears to be invalid or expired.\n\n"
              else:
                  summary += f"**Status:** Error - {first_error[:200]}\n\n"
          elif error:
              error_msg = error.get('message', str(error)) if isinstance(error, dict) else str(error)
              summary += f"**Status:** Error - {error_msg}\n\n"
          elif total > 0 and correct + incorrect > 0:
              pct = (correct / total) * 100
              summary += f"**Score:** {correct}/{total} ({pct:.1f}%)\n\n"
          else:
              summary += f"**Status:** {status}\n\n"

          if samples:
              summary += "| Sample | Category | Expected | Result |\n"
              summary += "|--------|----------|----------|--------|\n"

              for sample in samples:
                  sample_id = sample.get("id", "unknown")
                  metadata = sample.get("metadata", {})
                  category = metadata.get("category", "unknown")
                  expected = metadata.get("expected_diagnosis", "unknown")

                  # Check for sample error first
                  sample_error = sample.get("error")
                  if sample_error:
                      result_icon = "ðŸ”´ Error"
                  else:
                      score = sample.get("scores", {}).get("model_graded_fact", {})
                      value = score.get("value", "?")

                      if value == "C":
                          result_icon = "âœ…"
                      elif value == "I":
                          result_icon = "âŒ"
                      else:
                          result_icon = "âš ï¸"

                  summary += f"| {sample_id} | {category} | {expected} | {result_icon} |\n"

          summary += f"\n*Eval run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

          # Write to GitHub Actions summary
          with open(os.environ.get("GITHUB_STEP_SUMMARY", "/dev/null"), "a") as f:
              f.write(summary)

          # Write to file for PR comment
          with open("summary.md", "w") as f:
              f.write(summary)

          print(summary, flush=True)

          # Exit with error if evals had errors or all samples failed
          if sample_errors or error:
              print("Eval run had errors - failing the job", flush=True)
              exit(1)
          elif total > 0 and incorrect > 0 and correct == 0:
              print("All eval samples failed - failing the job", flush=True)
              exit(1)
          PYTHON_SCRIPT

      - name: Upload eval logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: inspect-eval-logs
          path: |
            evals/logs/
            evals/summary.md
          if-no-files-found: ignore

      - name: Find existing comment
        if: github.event_name == 'pull_request' && always()
        uses: peter-evans/find-comment@v3
        id: find-comment
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-includes: "## DNS Troubleshooter Eval Results"

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-path: evals/summary.md
          comment-id: ${{ steps.find-comment.outputs.comment-id }}
          edit-mode: replace
