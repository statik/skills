name: Validate Skills

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

permissions:
  contents: read
  pull-requests: write

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install skills-ref
        run: |
          pip install skills-ref
          echo "Installed to: $(pip show skills-ref | grep Location)"

      - name: Find and validate skills
        run: |
          exit_code=0
          for skill_dir in $(find . -name "SKILL.md" -exec dirname {} \; | sort -u); do
            echo "Validating: $skill_dir"
            if ! python -c "import sys; sys.argv = ['skills-ref', 'validate', '$skill_dir']; from skills_ref.cli import main; main()"; then
              exit_code=1
            fi
          done
          exit $exit_code

  run-evals:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Set up Python
        run: uv python install 3.11

      - name: Install dependencies
        working-directory: evals
        run: uv sync

      - name: Run InspectAI evals
        working-directory: evals
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Start DNS server in background and run evals
          uv run python << 'PYTHON_SCRIPT'
          import subprocess
          import time
          import json
          import os
          import sys
          from pathlib import Path

          from dns_server import TestDNSServer
          from test_zones import get_all_zones

          # Start DNS server
          zones = get_all_zones()
          server = TestDNSServer(zones, port=5053)
          server.start()
          time.sleep(0.5)
          print("DNS server started on port 5053")

          try:
              # Run the InspectAI eval
              result = subprocess.run(
                  ["uv", "run", "inspect", "eval", "dns_skill_eval.py", "--model", "anthropic/claude-sonnet-4-20250514"],
                  capture_output=True,
                  text=True,
                  timeout=600
              )
              print(result.stdout)
              if result.stderr:
                  print(result.stderr, file=sys.stderr)

              eval_exit_code = result.returncode
          finally:
              server.stop()
              print("DNS server stopped")

          sys.exit(eval_exit_code)
          PYTHON_SCRIPT

      - name: Parse and summarize eval results
        if: always()
        working-directory: evals
        run: |
          uv run python << 'PYTHON_SCRIPT'
          import json
          import os
          from pathlib import Path
          from datetime import datetime

          # Find the most recent log file
          log_dir = Path(".inspect")
          if not log_dir.exists():
              log_dir = Path.home() / ".inspect" / "logs"

          log_files = list(log_dir.rglob("*.json")) if log_dir.exists() else []

          if not log_files:
              summary = "## DNS Troubleshooter Eval Results\n\n"
              summary += "**Status:** No eval logs found\n"
              with open(os.environ.get("GITHUB_STEP_SUMMARY", "summary.md"), "a") as f:
                  f.write(summary)
              with open("summary.md", "w") as f:
                  f.write(summary)
              exit(0)

          # Get the most recent log
          log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
          log_file = log_files[0]

          print(f"Reading log file: {log_file}")

          with open(log_file) as f:
              log_data = json.load(f)

          # Extract results
          results = log_data.get("results", {})
          samples = log_data.get("samples", [])

          total = len(samples)
          correct = sum(1 for s in samples if s.get("scores", {}).get("model_graded_fact", {}).get("value") == "C")
          incorrect = sum(1 for s in samples if s.get("scores", {}).get("model_graded_fact", {}).get("value") == "I")

          # Build summary
          summary = "## DNS Troubleshooter Eval Results\n\n"

          if total > 0:
              pct = (correct / total) * 100
              summary += f"**Score:** {correct}/{total} ({pct:.1f}%)\n\n"
          else:
              summary += "**Score:** No samples evaluated\n\n"

          summary += "| Sample | Category | Expected | Result |\n"
          summary += "|--------|----------|----------|--------|\n"

          for sample in samples:
              sample_id = sample.get("id", "unknown")
              metadata = sample.get("metadata", {})
              category = metadata.get("category", "unknown")
              expected = metadata.get("expected_diagnosis", "unknown")

              score = sample.get("scores", {}).get("model_graded_fact", {})
              value = score.get("value", "?")

              if value == "C":
                  result_icon = "✅"
              elif value == "I":
                  result_icon = "❌"
              else:
                  result_icon = "⚠️"

              summary += f"| {sample_id} | {category} | {expected} | {result_icon} |\n"

          summary += f"\n*Eval run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"

          # Write to GitHub Actions summary
          with open(os.environ.get("GITHUB_STEP_SUMMARY", "/dev/null"), "a") as f:
              f.write(summary)

          # Write to file for PR comment
          with open("summary.md", "w") as f:
              f.write(summary)

          print(summary)
          PYTHON_SCRIPT

      - name: Upload eval logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: inspect-eval-logs
          path: |
            evals/.inspect/
            evals/summary.md
          if-no-files-found: ignore

      - name: Find existing comment
        if: github.event_name == 'pull_request' && always()
        uses: peter-evans/find-comment@v3
        id: find-comment
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-includes: "## DNS Troubleshooter Eval Results"

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-path: evals/summary.md
          comment-id: ${{ steps.find-comment.outputs.comment-id }}
          edit-mode: replace
